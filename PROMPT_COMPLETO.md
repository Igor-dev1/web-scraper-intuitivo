# ğŸ¯ PROMPT COMPLETO: Web Scraper Intuitivo com IA, Proxy e AutomaÃ§Ã£o

## ğŸ“‹ VisÃ£o Geral Completa

Crie uma aplicaÃ§Ã£o web de scraping profissional em Python usando **Streamlit** (frontend) e **Flask** (proxy server), com interface 100% em portuguÃªs brasileiro. O sistema deve permitir extraÃ§Ã£o de dados de qualquer site atravÃ©s de mÃºltiplos mÃ©todos, incluindo IA, com automaÃ§Ã£o completa e notificaÃ§Ãµes por email.

---

## âš ï¸ CONCEITO FUNDAMENTAL: CARREGAMENTO â‰  EXTRAÃ‡ÃƒO

### ğŸš¨ ATENÃ‡ÃƒO: NÃƒO CONFUNDIR OS CONCEITOS!

O sistema funciona em **2 ETAPAS COMPLETAMENTE SEPARADAS**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ETAPA 1: CARREGAR HTML (Como pegar a pÃ¡gina)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ âš¡ RÃ¡pido (Python): requests.get()                   â”‚
â”‚  â€¢ ğŸŒ Proxy CORS: Flask + corsproxy.io                  â”‚
â”‚  â€¢ ğŸ“ Upload HTML: Arquivo salvo do navegador           â”‚
â”‚                                                          â”‚
â”‚  RESULTADO: HTML em memÃ³ria (st.session_state)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ETAPA 2: EXTRAIR DADOS (Como coletar informaÃ§Ãµes)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ğŸ“Š Estrutura HTML: Ver tags/classes/IDs              â”‚
â”‚  â€¢ ğŸ¤– ExtraÃ§Ã£o com IA: IA identifica seletores          â”‚
â”‚  â€¢ ğŸš€ Scraping em Massa: Aplicar em mÃºltiplas URLs      â”‚
â”‚  â€¢ âš¡ Validador: Testar seletores manualmente           â”‚
â”‚  â€¢ ğŸ¤– AutomaÃ§Ã£o: Agendar scraping periÃ³dico             â”‚
â”‚                                                          â”‚
â”‚  RESULTADO: Dados extraÃ­dos em CSV/JSON                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âŒ ERRO COMUM:

**ERRADO**: "A IA carrega a pÃ¡gina"
**CERTO**: "A IA identifica seletores no HTML JÃ CARREGADO"

**ERRADO**: "Proxy Ã© para extrair dados"
**CERTO**: "Proxy Ã© para CARREGAR HTML de sites que bloqueiam Python"

**ERRADO**: "Upload de HTML extrai automaticamente"
**CERTO**: "Upload CARREGA o HTML, depois vocÃª escolhe ABA para extrair"

### âœ… FLUXO CORRETO:

1. **Primeiro**: Escolha COMO carregar (Python/Proxy/Upload)
2. **Depois**: Escolha ABA para extrair (IA/Massa/Validador/etc)

---

## ğŸ” AUTENTICAÃ‡ÃƒO E SEGURANÃ‡A

### Sistema de Login Customizado
- **AutenticaÃ§Ã£o simples** com usuÃ¡rio/senha via Streamlit
- Credenciais configurÃ¡veis via **Replit Secrets**:
  - `ADMIN_USERNAME` (padrÃ£o: "admin")
  - `ADMIN_PASSWORD` (padrÃ£o: "admin123")
- **Tela de login** com formulÃ¡rio antes de qualquer acesso
- **BotÃ£o de logout** no sidebar
- **Session state** para manter usuÃ¡rio logado durante a sessÃ£o
- **Email admin master**: igor.as0703@gmail.com (hardcoded)

### Gerenciamento de Acessos (Admin)
- Painel no sidebar: "ğŸ‘‘ Gerenciar Acessos (Admin)"
- **Adicionar usuÃ¡rios**: Campo de email + botÃ£o "Adicionar Acesso"
- **Remover usuÃ¡rios**: BotÃ£o ğŸ—‘ï¸ ao lado de cada email
- **Lista de usuÃ¡rios autorizados**: Visual com indicadores ğŸ‘‘ (admin) e âœ‰ï¸ (usuÃ¡rios)
- **ProteÃ§Ã£o**: Admin master nÃ£o pode ser removido
- **PersistÃªncia**: Arquivo `authorized_emails.json` (excluÃ­do do git)

---

## ğŸ”‘ GERENCIAMENTO DE API KEYS

### Sistema HÃ­brido (Replit Secrets + Custom)
- **Prioridade**: Replit Secrets > API Keys Customizadas
- **Painel Admin** no sidebar: "ğŸ”‘ Gerenciar API Keys"

#### Adicionar Keys Customizadas:
- FormulÃ¡rio: Nome da Key + Valor
- BotÃ£o "Salvar API Key"
- **Mascaramento**: Exibe apenas inÃ­cio/fim (ex: `sk-...xyz`)
- **Indicadores**: ğŸ”’ (Replit Secrets) vs ğŸ”‘ (Custom)

#### Remover Keys:
- BotÃ£o ğŸ—‘ï¸ ao lado de cada key
- ConfirmaÃ§Ã£o antes de excluir

#### PersistÃªncia:
- Arquivo `api_keys.json` (excluÃ­do do git)
- FunÃ§Ã£o `get_api_key(name)`: busca em Secrets primeiro, depois em custom

#### Provedores Suportados:
- `OPENAI_API_KEY` - OpenAI GPT-5
- `ANTHROPIC_API_KEY` - Anthropic Claude Sonnet 4
- `GEMINI_API_KEY` - Google Gemini 2.5 Flash

---

## ğŸŒ PROXY SERVER (Flask - Porta 5001)

### Arquivo: `proxy_server.py`
- **Framework**: Flask com CORS habilitado
- **Porta**: 5001 (5000 Ã© do Streamlit)
- **Host**: 0.0.0.0 (acessÃ­vel externamente)

### Endpoints:

#### 1. `/proxy?url=<URL>`
- **MÃ©todo**: GET
- **FunÃ§Ã£o**: Proxy CORS usando corsproxy.io
- **Headers**: User-Agent customizado (navegador real)
- **Timeout**: 15 segundos
- **Retorno**: HTML da pÃ¡gina com headers CORS
- **Uso**: Contornar bloqueios CORS e anti-scraping

#### 2. `/health`
- **MÃ©todo**: GET
- **FunÃ§Ã£o**: Health check do servidor
- **Retorno**: `{"status": "ok"}`

### Workflow AutomÃ¡tico:
- Comando: `python proxy_server.py`
- Nome: "Proxy Server"
- Inicia automaticamente com o projeto

---

## ğŸ•·ï¸ APLICAÃ‡ÃƒO STREAMLIT (Porta 5000)

### Arquivo: `app.py` (~1760 linhas)

### ConfiguraÃ§Ã£o Inicial:
```python
st.set_page_config(
    page_title="Web Scraper Intuitivo",
    page_icon="ğŸ•·ï¸",
    layout="wide"
)
```

---

## ğŸ“‚ SIDEBAR - PAINEL DE CONTROLE

### âš ï¸ CONCEITOS SEPARADOS: CARREGAMENTO vs EXTRAÃ‡ÃƒO

**Ã‰ FUNDAMENTAL entender que hÃ¡ 2 etapas INDEPENDENTES:**

#### ğŸ”µ ETAPA 1: CARREGAR O HTML (Como buscar a pÃ¡gina)
**Escolha COMO pegar o HTML:**
- **âš¡ RÃ¡pido (Python)**: RequisiÃ§Ã£o HTTP direta com `requests`
- **ğŸŒ Proxy CORS**: Via servidor Flask + corsproxy.io
- **ğŸ“ Upload de HTML**: Arquivo salvo do navegador

#### ğŸŸ¢ ETAPA 2: EXTRAIR DADOS (Como identificar e coletar informaÃ§Ãµes)
**Escolha COMO extrair os dados do HTML jÃ¡ carregado:**
- **Estrutura HTML**: Ver tags, classes, IDs
- **ExtraÃ§Ã£o com IA**: IA identifica seletores CSS/XPath
- **Scraping em Massa**: Aplicar seletores em mÃºltiplas pÃ¡ginas
- **Validador**: Testar seletores manualmente
- **AutomaÃ§Ã£o**: Agendar scraping periÃ³dico

---

### 1. ETAPA 1 - ConfiguraÃ§Ãµes de Carregamento

#### A. MÃ©todo de Carregamento (COMO BUSCAR O HTML):

**OpÃ§Ã£o 1: âš¡ RÃ¡pido (Python)**
- Usa biblioteca `requests` do Python
- Headers customizados (User-Agent, Accept-Language, etc)
- Timeout: 10 segundos
- Cookies especiais para Steam (verificaÃ§Ã£o de idade)
- **QUANDO USAR**: Sites normais, blogs, lojas simples
- **NÃƒO FUNCIONA**: Sites com JavaScript pesado (Steam, Amazon)

**OpÃ§Ã£o 2: ğŸŒ Proxy CORS**
- Usa servidor Flask na porta 5001
- Proxy intermediÃ¡rio: corsproxy.io
- Contorna bloqueios CORS e anti-scraping
- Timeout: 20 segundos
- **QUANDO USAR**: Sites que bloqueiam requisiÃ§Ãµes Python
- **LIMITAÃ‡ÃƒO**: Pode falhar se JavaScript carregar dados dinamicamente

**OpÃ§Ã£o 3: ğŸ“ Upload de arquivo HTML**
- Salve a pÃ¡gina no navegador (Ctrl+S)
- Ou visualize cÃ³digo (Ctrl+U) e salve
- Upload do arquivo .html
- **QUANDO USAR**: Sites JavaScript-heavy (Steam, Amazon, SPAs)
- **VANTAGEM**: HTML completo renderizado pelo navegador

#### B. Carregar a PÃ¡gina:
- **ğŸ“¡ Carregar de URL**: 
  - Campo de input para URL
  - BotÃ£o "ğŸ” Carregar PÃ¡gina"
  - Usa mÃ©todo escolhido (Python ou Proxy)
  
- **ğŸ“ Upload de arquivo HTML**: 
  - File uploader (.html, .htm)
  - BotÃ£o "ğŸ“¥ Processar HTML"
  - Ignora mÃ©todo de carregamento (lÃª arquivo direto)

#### C. Download do HTML:
- BotÃ£o: "ğŸ’¾ Baixar HTML da PÃ¡gina"
- Salva HTML atual em arquivo .html
- **ÃšTIL PARA**: 
  - Backup da pÃ¡gina
  - Processar offline depois
  - Compartilhar HTML com outras pessoas

### 2. PainÃ©is de AdministraÃ§Ã£o (apenas admin)

#### ğŸ‘‘ Gerenciar Acessos
- Adicionar emails autorizados
- Remover usuÃ¡rios (exceto admin master)
- Visualizar lista completa

#### ğŸ”‘ Gerenciar API Keys
- Adicionar keys customizadas
- Visualizar keys (mascaradas)
- Remover keys
- Ver origem (Secrets vs Custom)

### 3. HistÃ³rico de ExecuÃ§Ãµes (Scraping AutomÃ¡tico)
- **Ãšltimas 5 execuÃ§Ãµes** de tarefas automÃ¡ticas
- Exibe: Nome, Status, Data/Hora, Produtos encontrados
- Indicadores visuais: âœ… Sucesso / âŒ Erro

### 4. BotÃ£o de Logout
- ğŸšª Sair (disponÃ­vel apÃ³s login)

---

## ğŸ“‘ ETAPA 2 - SISTEMA DE ABAS (5 MÃ‰TODOS DE EXTRAÃ‡ÃƒO)

### âš ï¸ IMPORTANTE: As abas sÃ£o MÃ‰TODOS DIFERENTES de EXTRAIR dados

**Depois de CARREGAR o HTML (Etapa 1), escolha UMA das abas para extrair:**

1. **ğŸ“Š Estrutura HTML**: Ver o que tem na pÃ¡gina (tags, classes, IDs)
2. **ğŸ¤– ExtraÃ§Ã£o com IA**: IA identifica seletores CSS/XPath automaticamente
3. **ğŸš€ Scraping em Massa**: Aplicar seletores em mÃºltiplas URLs/HTMLs
4. **âš¡ Validador de Seletores**: Testar vÃ¡rios seletores de uma vez
5. **ğŸ¤– Scraping AutomÃ¡tico**: Agendar scraping periÃ³dico com email

**TODAS AS ABAS TRABALHAM COM O MESMO HTML JÃ CARREGADO!**

---

### ğŸ“Š TAB 1: Estrutura HTML

**Objetivo**: Visualizar estrutura completa da pÃ¡gina para identificar elementos.

**âš ï¸ ESTA ABA NÃƒO USA IA! Apenas mostra o HTML carregado na Etapa 1.**

#### EstatÃ­sticas da PÃ¡gina:
- Total de elementos HTML
- Tipos de tags Ãºnicos
- Contagem especÃ­fica: Links (a), Imagens (img), Divs, ParÃ¡grafos (p), TÃ­tulos (h1-h6)
- ExibiÃ§Ã£o em tabela interativa

#### Tags DisponÃ­veis:
- Lista ordenada alfabeticamente de todas as tags HTML encontradas
- Text area com todas as tags separadas por vÃ­rgula

#### Classes CSS DisponÃ­veis:
- ExtraÃ§Ã£o automÃ¡tica de todas as classes CSS da pÃ¡gina
- Ordenadas alfabeticamente
- Text area para fÃ¡cil busca/cÃ³pia

#### IDs DisponÃ­veis:
- Lista de todos os IDs encontrados na pÃ¡gina
- Ordenados alfabeticamente
- Text area para referÃªncia

#### PrÃ©via do HTML:
- Primeiros 5000 caracteres do HTML formatado (prettify)
- Syntax highlighting para HTML
- Code block interativo

#### VisualizaÃ§Ã£o Melhorada:
- RenderizaÃ§Ã£o visual do HTML com CSS customizado:
  - `display: block` para elementos de bloco
  - `clear: both` para evitar sobreposiÃ§Ã£o
  - EspaÃ§amento correto (h1-h6, p, img)
  - **Imagens HD**: Prioriza `data-src`, `data-screenshot`, depois `src`
  - Imagens inline quando apropriado (detecÃ§Ã£o por keywords)
- Height: 600px scrollÃ¡vel
- Unsafe HTML habilitado para renderizaÃ§Ã£o completa

---

### ğŸ¤– TAB 2: ExtraÃ§Ã£o com IA

**Objetivo**: Identificar seletores automaticamente usando IA com descriÃ§Ã£o em linguagem natural.

**âš ï¸ IMPORTANTE**: 
- A IA analisa o HTML **JÃ CARREGADO na Etapa 1**
- NÃ£o faz requisiÃ§Ã£o nova - trabalha com o HTML em memÃ³ria
- Identifica CSS/XPath para extrair dados especÃ­ficos
- **NÃƒO Ã‰** um mÃ©todo de carregamento - Ã© um mÃ©todo de EXTRAÃ‡ÃƒO

#### SeleÃ§Ã£o de Provedor de IA:
- **OpenAI (ChatGPT)**
  - Modelo: **gpt-5** (mais recente - agosto 2025)
  - Link para keys: https://platform.openai.com/api-keys
  
- **Anthropic (Claude)**
  - Modelo: **Claude Sonnet 4**
  - Link para keys: https://console.anthropic.com/settings/keys
  
- **Google (Gemini)**
  - Modelo: **gemini-2.5-flash** (mais recente)
  - Link para keys: https://ai.google.dev/gemini-api/docs/api-key

#### ConfiguraÃ§Ã£o de API Key:
- Detecta se existe key salva (Secrets ou Custom)
- Mostra origem da key: ğŸ”’ (Secrets) ou ğŸ”‘ (Custom)
- Checkbox: "Usar API Key salva" (padrÃ£o: marcado)
- Input manual opcional se desmarcar
- Link para adicionar em "Gerenciar API Keys"

#### DescriÃ§Ã£o do que Extrair:
- **Text area** grande para descriÃ§Ã£o em linguagem natural
- **Placeholder com exemplos**:
  ```
  Exemplo 1: "Encontre o tÃ­tulo do jogo, preÃ§o e link para compra"
  Exemplo 2: "Extraia nome do produto, avaliaÃ§Ã£o em estrelas e disponibilidade"
  Exemplo 3: "Busque todos os artigos com data de publicaÃ§Ã£o e autor"
  ```
- BotÃ£o: **"ğŸ” Buscar com IA"**

#### Formato de Resposta da IA:
A IA retorna JSON estruturado:
```json
{
  "seletores": [
    {
      "campo": "TÃ­tulo do Produto",
      "seletor": "h1.product-title",
      "tipo": "CSS",
      "explicacao": "Tag h1 com classe product-title contÃ©m o tÃ­tulo principal",
      "exemplo": "Smartphone XYZ 128GB"
    },
    {
      "campo": "PreÃ§o",
      "seletor": "//span[@class='price']/text()",
      "tipo": "XPath",
      "explicacao": "Span com classe price contÃ©m o valor monetÃ¡rio",
      "exemplo": "R$ 1.499,90"
    }
  ]
}
```

#### ExibiÃ§Ã£o de Resultados:
1. **Tabela consolidada** com dados extraÃ­dos usando TODOS os seletores identificados
2. **Download direto**: CSV e JSON dos resultados
3. **Expander "Detalhes dos Seletores"**:
   - Tabela com: Campo, Seletor, Tipo, ExplicaÃ§Ã£o, Exemplo
   - BotÃ£o: **"ğŸ“‹ Copiar Seletores"** - copia APENAS as strings dos seletores (um por linha, sem comentÃ¡rios)
4. **BotÃ£o "Limpar"**: Limpa resultados e permite nova consulta
5. **PersistÃªncia**: Resultados salvos em `st.session_state.ai_result` (mantÃ©m entre reloads)

#### IntegraÃ§Ã£o com Outras Abas:
- Seletores ficam disponÃ­veis para uso em **Scraping em Massa**
- Checkbox automÃ¡tico: "âœ¨ Usar seletores identificados pela IA"

---

### ğŸš€ TAB 3: Scraping em Massa

**Objetivo**: Extrair dados de mÃºltiplas URLs ou arquivos HTML usando mesmos seletores.

#### Seletores da IA (se disponÃ­veis):
- Banner de sucesso: "ğŸ¤– Seletores da IA disponÃ­veis!"
- Checkbox: **"âœ¨ Usar seletores identificados pela IA"** (padrÃ£o: marcado)
- Se marcado, aplica TODOS os seletores da IA automaticamente

#### MÃ©todo de Entrada:
**OpÃ§Ã£o 1: ğŸ“ Inserir URLs**
- Text area para mÃºltiplas URLs (uma por linha)
- Placeholder com exemplos de URLs
- Processo: Faz requisiÃ§Ã£o HTTP para cada URL

**OpÃ§Ã£o 2: ğŸ“„ Upload de Arquivos HTML**
- Upload mÃºltiplo de arquivos .html/.htm
- Exibe nÃºmero de arquivos + nomes
- **Ideal para sites JavaScript-heavy** (Steam, Amazon, etc):
  - Salve a pÃ¡gina com Ctrl+S (navegador)
  - Ou Ctrl+U â†’ Save (HTML puro)
  - Upload do arquivo salvo
  - Aplica seletores de IA em todos os arquivos
- Resultados mostram nome do arquivo de origem

#### Seletores Customizados (se nÃ£o usar IA):
**MÃ©todos disponÃ­veis**:
1. **Seletor CSS**: `div.produto`, `#header > p`
2. **XPath**: `//div[@class='produto']`, `//a/@href`
3. **Tag HTML**: `h1`, `div`, `span`
4. **Classe CSS**: `produto`, `price`

#### OpÃ§Ãµes de ExtraÃ§Ã£o:
- Checkbox: **"Extrair texto"** (padrÃ£o: marcado)
- Checkbox: **"Extrair atributos"** (href, src, alt, title, etc)

#### Processamento:
- BotÃ£o: **"ğŸš€ Iniciar Scraping em Massa"**
- **Progress bar** visual com contagem
- Para cada URL/arquivo:
  1. Carrega conteÃºdo (requisiÃ§Ã£o ou arquivo)
  2. Parseia com BeautifulSoup
  3. Aplica seletores (IA ou custom)
  4. Coleta dados
  5. Adiciona coluna "URL Origem" ou "Arquivo Origem"

#### Resultados:
- **Tabela consolidada** com todos os dados extraÃ­dos
- Colunas: URL/Arquivo + campos extraÃ­dos
- **Downloads**:
  - ğŸ“¥ CSV: `scraping_massa.csv`
  - ğŸ“¥ JSON: `scraping_massa.json`

---

### âš¡ TAB 4: Validador de Seletores

**Objetivo**: Testar mÃºltiplos seletores (CSS + XPath) simultaneamente na pÃ¡gina carregada.

#### Input de Seletores:
- **Text area grande** para colar seletores
- **Um seletor por linha**
- **Aceita mistura** de CSS e XPath
- **Placeholder com exemplos**:
  ```
  div.apphub_AppName
  //div[@class='produto']/h1
  span.price
  //a/@href
  ```

#### OpÃ§Ãµes de ExtraÃ§Ã£o:
- Checkbox: **"Extrair texto"** (padrÃ£o: marcado)
- Checkbox: **"Extrair atributos"**

#### DetecÃ§Ã£o AutomÃ¡tica de Tipo:
A ferramenta identifica automaticamente se Ã© CSS ou XPath:
- **Indicadores de XPath**:
  - ComeÃ§a com `//`, `/`, `./`, `(//`, `(./`
  - ContÃ©m `::` (eixos como `descendant::`, `child::`)
  - ContÃ©m `@` com `/` (atributos com path)
- **Se nÃ£o for XPath**: considera CSS

#### Processamento:
- BotÃ£o: **"ğŸš€ Testar Todos os Seletores"**
- Para cada seletor:
  1. Detecta tipo (XPath vs CSS)
  2. Aplica seletor na pÃ¡gina
  3. Conta elementos encontrados
  4. Extrai primeiro valor (preview)
  5. Trata erros (exibe mensagem)

#### Resultados:
- **Tabela com 4 colunas**:
  1. **Seletor**: String original
  2. **Tipo**: "XPath" ou "CSS"
  3. **Encontrados**: NÃºmero de elementos
  4. **Primeiro Valor**: Preview do primeiro resultado
- Indicadores visuais:
  - âœ… Verde: Encontrou elementos
  - âš ï¸ Amarelo: Nenhum elemento encontrado
  - âŒ Vermelho: Erro no seletor

#### Downloads:
- ğŸ“¥ CSV: `teste_universal.csv`
- ğŸ“¥ JSON: `teste_universal.json`

---

### ğŸ¤– TAB 5: Scraping AutomÃ¡tico

**Objetivo**: Configurar tarefas de scraping periÃ³dicas com agendamento e notificaÃ§Ãµes por email.

**âš ï¸ ACESSO RESTRITO**: Apenas administradores

#### Banner Informativo:
```
ğŸ’¡ ExecuÃ§Ã£o Manual: Use o botÃ£o â–¶ï¸ para executar tarefas sob demanda.
Para agendamento automÃ¡tico, configure Replit Scheduled Deployments no painel de deployment.
```

#### Layout: 2 Colunas

---

##### COLUNA 1: Nova Tarefa de Scraping

**FormulÃ¡rio em 5 Etapas:**

**1ï¸âƒ£ ConfiguraÃ§Ã£o da Fonte**
- **Nome da Tarefa**: Input text (ex: "LanÃ§amentos Steam Semanal")
- **URL da PÃ¡gina de LanÃ§amentos**: Input text com placeholder

**2ï¸âƒ£ ConfiguraÃ§Ã£o de Busca**
- **Site Alvo para Buscar Produtos**: Input text
- **MÃ©todo de Busca**: Selectbox com 3 opÃ§Ãµes:
  - "HÃ­brido (Python + IA)"
  - "Apenas Python"
  - "Apenas IA"

**3ï¸âƒ£ Campos para Extrair**
- **Text area** para campos desejados (um por linha)
- **Placeholder**:
  ```
  TÃ­tulo
  PreÃ§o
  Disponibilidade
  Link
  Imagem
  ```

**4ï¸âƒ£ Agendamento**
- **FrequÃªncia**: Selectbox
  - "DiÃ¡rio"
  - "Semanal"
  - "Mensal"
  - "Personalizado"
- **Se Personalizado**: Input para cron expression (ex: `0 10 * * 1`)

**5ï¸âƒ£ NotificaÃ§Ãµes por Email**

**Provedor de Email**: Selectbox com 4 opÃ§Ãµes:

**A. SMTP Customizado** (Totalmente Funcional)
- **Servidor SMTP**: Input (ex: `smtp.gmail.com`)
- **Porta**: Number input (ex: `587`)
- **UsuÃ¡rio (Email Remetente)**: Input
- **Senha**: Password input
- **Email DestinatÃ¡rio**: Input
- **Funcionamento**:
  - Suporta Gmail, Outlook, Yahoo, qualquer SMTP
  - Usa `smtplib` do Python
  - TLS habilitado
  - Testa conexÃ£o antes de salvar

**B. SendGrid** (IntegraÃ§Ã£o Preparada)
- **API Key**: Password input
- **Email DestinatÃ¡rio**: Input
- **Status**: Aviso de implementaÃ§Ã£o

**C. Resend** (IntegraÃ§Ã£o Preparada)
- **API Key**: Password input
- **Email DestinatÃ¡rio**: Input
- **Status**: Aviso de implementaÃ§Ã£o

**D. Gmail** (IntegraÃ§Ã£o Preparada)
- **Email Remetente (Gmail)**: Input
- **Senha de Aplicativo**: Password input
- **Email DestinatÃ¡rio**: Input
- **Status**: Aviso de implementaÃ§Ã£o

**BotÃ£o Final**:
- **"ğŸ’¾ Salvar Tarefa"**: Submit button (full width)

---

##### COLUNA 2: Tarefas Ativas

**Lista de Tarefas Configuradas:**

Para cada tarefa, exibe **card expansÃ­vel**:
- **Nome da tarefa** (tÃ­tulo do expander)
- **ConteÃºdo expandido**:
  ```
  ğŸ¯ Fonte: [URL]
  ğŸ” Site Alvo: [URL]
  ğŸ§  MÃ©todo: [HÃ­brido/Python/IA]
  ğŸ“‹ Campos: [lista]
  â° FrequÃªncia: [DiÃ¡rio/Semanal/etc]
  ğŸ“§ Email: [provedor] â†’ [destinatÃ¡rio]
  ```

**AÃ§Ãµes por Tarefa**:
- **â–¶ï¸ Executar Agora**: Button (largura completa)
  - Executa scraping imediatamente
  - Exibe spinner durante execuÃ§Ã£o
  - **Mostra preview dos produtos**:
    - Tabela com primeiros resultados
    - Contagem total de produtos
  - Salva execuÃ§Ã£o em histÃ³rico
  
- **ğŸ—‘ï¸ Deletar**: Button (largura completa)
  - Remove tarefa permanentemente
  - Atualiza lista

#### Armazenamento:

**Arquivo 1: `scraping_tasks.json`**
Estrutura:
```json
{
  "task_id_uuid": {
    "id": "uuid-gerado",
    "name": "Nome da Tarefa",
    "source_url": "https://...",
    "target_site": "https://...",
    "search_method": "HÃ­brido (Python + IA)",
    "fields": ["TÃ­tulo", "PreÃ§o", "Link"],
    "frequency": "Semanal",
    "custom_schedule": "",
    "email_config": {
      "provider": "SMTP Customizado",
      "smtp_server": "smtp.gmail.com",
      "smtp_port": 587,
      "smtp_user": "user@gmail.com",
      "smtp_password": "senha_criptografada",
      "recipient": "destino@email.com"
    },
    "created_at": "2025-10-27T20:00:00"
  }
}
```

**Arquivo 2: `scraping_history.json`**
Estrutura:
```json
[
  {
    "task_id": "uuid-da-tarefa",
    "task_name": "Nome da Tarefa",
    "status": "success",
    "timestamp": "2025-10-27T21:30:00",
    "products_found": 15,
    "error_message": null,
    "data_preview": [
      {"TÃ­tulo": "Produto 1", "PreÃ§o": "R$ 99"},
      {"TÃ­tulo": "Produto 2", "PreÃ§o": "R$ 149"}
    ]
  }
]
```

**ExclusÃ£o do Git**: Ambos arquivos em `.gitignore`

#### HistÃ³rico de ExecuÃ§Ãµes (Sidebar):
- **SeÃ§Ã£o no sidebar**: "ğŸ“Š HistÃ³rico de ExecuÃ§Ãµes"
- **Ãšltimas 5 execuÃ§Ãµes**
- Para cada execuÃ§Ã£o:
  ```
  [Nome da Tarefa]
  âœ… Sucesso / âŒ Erro
  ğŸ• 27/10/2025 21:30
  ğŸ“¦ 15 produtos encontrados
  ```
- Ordenado por mais recente

---

## ğŸ§© FUNÃ‡Ã•ES AUXILIARES IMPORTANTES

### 1. ExtraÃ§Ã£o de Dados

#### `extract_with_css(selector, extract_text, extract_attrs)`
- Usa `BeautifulSoup.select()`
- Retorna lista de dicionÃ¡rios
- Colunas: "Texto", "Atributos"

#### `extract_with_xpath(xpath, extract_text, extract_attrs)`
- Usa `lxml_html.fromstring()` e `.xpath()`
- Detecta se Ã© atributo ou elemento
- Retorna lista de dicionÃ¡rios

#### `extract_with_tag(tag_name)`
- Usa `BeautifulSoup.find_all(tag)`
- Retorna textos extraÃ­dos

#### `extract_with_class(class_name)`
- Usa `BeautifulSoup.find_all(class_=class_name)`
- Retorna textos extraÃ­dos

### 2. IA e Seletores

#### `ask_ai_for_selectors(html_content, user_query, ai_provider, api_key)`
- Monta prompt detalhado para IA
- **Prompt inclui**:
  - HTML completo da pÃ¡gina
  - Requisitos de resposta JSON
  - Exemplos de seletores CSS e XPath
  - InstruÃ§Ãµes para explicaÃ§Ã£o e exemplos
- **Retorna**: JSON com array de seletores
- **Tratamento de erros**: Anthropic concatena blocos de texto

#### `get_api_key(key_name)`
- Busca em Replit Secrets (`os.environ.get()`)
- Se nÃ£o encontrar, busca em custom keys (`api_keys.json`)
- Retorna valor ou None

### 3. Gerenciamento de Tarefas

#### `load_scraping_tasks()`
- Carrega `scraping_tasks.json`
- Retorna dicionÃ¡rio de tarefas

#### `save_scraping_tasks(tasks)`
- Salva dicionÃ¡rio em `scraping_tasks.json`
- Formato JSON indentado

#### `execute_scraping_task(task)`
- Executa uma tarefa de scraping
- Passos:
  1. Faz requisiÃ§Ã£o HTTP para `source_url`
  2. Parseia HTML com BeautifulSoup
  3. Aplica mÃ©todo de busca (Python/IA/HÃ­brido)
  4. Extrai campos especificados
  5. Retorna lista de produtos
- **Retorna**: tupla `(success, data, error_message)`

#### `send_email_notification(email_config, task_name, products)`
- Monta email HTML formatado
- **Suporta SMTP customizado** (funcionando):
  ```python
  server = smtplib.SMTP(smtp_server, smtp_port)
  server.starttls()
  server.login(smtp_user, smtp_password)
  server.send_message(msg)
  ```
- **SendGrid/Resend/Gmail**: Preparado mas nÃ£o totalmente implementado

#### `save_execution_history(task_id, task_name, status, products_count, error_msg, data_preview)`
- Adiciona execuÃ§Ã£o ao histÃ³rico
- Salva em `scraping_history.json`
- MantÃ©m Ãºltimas 100 execuÃ§Ãµes

---

## ğŸ“¦ DEPENDÃŠNCIAS E ARQUIVOS

### Bibliotecas Python (pyproject.toml):
```toml
dependencies = [
    "anthropic",
    "apscheduler",
    "beautifulsoup4",
    "flask",
    "flask-cors",
    "google-genai",
    "lxml",
    "openai",
    "pandas",
    "replit",
    "requests",
    "selenium",
    "streamlit",
    "trafilatura",
    "webdriver-manager",
]
```

### Arquivos de Dados (excluÃ­dos do git):
1. **api_keys.json**: API keys customizadas
2. **authorized_emails.json**: Emails autorizados
3. **scraping_tasks.json**: Tarefas de scraping configuradas
4. **scraping_history.json**: HistÃ³rico de execuÃ§Ãµes

### ConfiguraÃ§Ã£o Streamlit (.streamlit/config.toml):
```toml
[server]
port = 5000
address = "0.0.0.0"
headless = true

[theme]
# Tema padrÃ£o do Streamlit
```

### .gitignore:
```
api_keys.json
authorized_emails.json
scraping_tasks.json
scraping_history.json
__pycache__/
*.pyc
.env
```

---

## ğŸš€ WORKFLOWS DE EXECUÃ‡ÃƒO

### Workflow 1: Proxy Server
```bash
python proxy_server.py
```
- Inicia Flask na porta 5001
- Sempre ativo

### Workflow 2: Server (Streamlit)
```bash
streamlit run app.py --server.port 5000 --server.address 0.0.0.0 --server.headless true
```
- Inicia Streamlit na porta 5000
- Interface principal

---

## ğŸ¨ CARACTERÃSTICAS TÃ‰CNICAS

### Headers HTTP (Requests):
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}
```

### Session State (Streamlit):
```python
st.session_state.html_content = "..."  # HTML bruto
st.session_state.soup = BeautifulSoup(...)  # Objeto parseado
st.session_state.url = "https://..."  # URL atual
st.session_state.ai_result = {...}  # Resultado da IA
st.session_state.authenticated = True/False  # Login
st.session_state.user_email = "..."  # Email do usuÃ¡rio
st.session_state.is_admin = True/False  # Se Ã© admin
```

### Tratamento de Erros:
- **HTTP**: `response.raise_for_status()`
- **IA**: Try/except com mensagens especÃ­ficas
- **Seletores**: ValidaÃ§Ã£o e mensagens de erro amigÃ¡veis
- **Email**: Testa conexÃ£o SMTP antes de salvar

---

## ğŸ“± INTERFACE EM PORTUGUÃŠS

### Exemplos de Textos:
- âœ… "Login realizado com sucesso!"
- âŒ "UsuÃ¡rio ou senha incorretos!"
- ğŸš€ "Iniciar Scraping em Massa"
- ğŸ“¥ "Download CSV"
- ğŸ’¡ "Credenciais padrÃ£o: usuÃ¡rio: admin | senha: admin123"
- âš ï¸ "Esta funcionalidade estÃ¡ disponÃ­vel apenas para administradores"
- ğŸ¤– "Seletores da IA disponÃ­veis!"

### Placeholders:
- "Digite seu usuÃ¡rio"
- "Digite sua senha"
- "https://exemplo.com"
- "Encontre o tÃ­tulo do jogo, preÃ§o e link para compra"

---

## ğŸ”’ SEGURANÃ‡A

1. **Senhas**: Type="password" em todos os inputs sensÃ­veis
2. **API Keys**: Mascaradas na exibiÃ§Ã£o (sk-...xyz)
3. **Session State**: Limpo ao fazer logout
4. **Admin Master**: Email hardcoded, nÃ£o pode ser removido
5. **SMTP**: Credenciais nÃ£o exibidas depois de salvar
6. **Git**: Arquivos sensÃ­veis em .gitignore

---

## ğŸ“Š FUNCIONALIDADES COMPLETAS

### MÃ©todos de ExtraÃ§Ã£o:
1. âœ… CSS Selectors
2. âœ… XPath
3. âœ… HTML Tags
4. âœ… CSS Classes
5. âœ… IDs (via CSS)
6. âœ… IA-Assisted (3 provedores)
7. âœ… Bulk Scraping (URLs + HTML files)
8. âœ… Universal Validator (mistura CSS + XPath)

### Provedores de IA:
1. âœ… OpenAI GPT-5
2. âœ… Anthropic Claude Sonnet 4
3. âœ… Google Gemini 2.5 Flash

### AutomaÃ§Ã£o:
1. âœ… ConfiguraÃ§Ã£o de tarefas via UI
2. âœ… ExecuÃ§Ã£o manual sob demanda
3. âœ… Agendamento (frequÃªncias prÃ©-definidas + cron custom)
4. âœ… Email notifications (SMTP custom funcional)
5. âœ… HistÃ³rico de execuÃ§Ãµes
6. âœ… Preview de resultados

### Proxy e Anti-Scraping:
1. âœ… Servidor Flask proxy (porta 5001)
2. âœ… IntegraÃ§Ã£o com corsproxy.io
3. âœ… Headers customizados (User-Agent, etc)
4. âœ… Upload de HTML para JS-heavy sites

---

## ğŸ¯ CASOS DE USO COMPLETOS

### âš ï¸ ATENÃ‡ÃƒO: Cada caso mostra ETAPA 1 (Carregar) + ETAPA 2 (Extrair)

---

### 1. Scraping Simples de Blog/NotÃ­cia:

**ğŸ”µ ETAPA 1 - CARREGAR HTML:**
1. Login (admin / admin123)
2. Sidebar: MÃ©todo = "âš¡ RÃ¡pido (Python)"
3. Sidebar: Inserir URL â†’ "ğŸ” Carregar PÃ¡gina"
4. âœ… HTML carregado em memÃ³ria

**ğŸŸ¢ ETAPA 2 - EXTRAIR DADOS:**
1. Aba "ğŸ“Š Estrutura HTML" â†’ Ver tags, classes disponÃ­veis
2. Aba "âš¡ Validador de Seletores" â†’ Testar seletores manualmente
3. Download CSV/JSON

---

### 2. Scraping com IA (Loja Online):

**ğŸ”µ ETAPA 1 - CARREGAR HTML:**
1. Login
2. Sidebar: MÃ©todo = "âš¡ RÃ¡pido (Python)" ou "ğŸŒ Proxy CORS"
3. Inserir URL da pÃ¡gina de produto
4. Clicar "ğŸ” Carregar PÃ¡gina"
5. âœ… HTML carregado

**ğŸŸ¢ ETAPA 2 - EXTRAIR DADOS:**
1. Aba "ğŸ¤– ExtraÃ§Ã£o com IA"
2. Escolher provedor (OpenAI, Claude, Gemini)
3. Adicionar API key (se necessÃ¡rio)
4. Descrever: "Extraia tÃ­tulo, preÃ§o, disponibilidade e imagem do produto"
5. Clicar "ğŸ” Buscar com IA"
6. IA identifica seletores CSS/XPath automaticamente
7. Resultados aparecem em tabela
8. Download CSV/JSON direto
9. Copiar seletores para reusar depois

---

### 3. Scraping em Massa (MÃºltiplos Produtos):

**ğŸ”µ ETAPA 1A - CARREGAR PÃGINA DE EXEMPLO:**
1. Login
2. Carregar UMA pÃ¡gina de produto (para treinar IA)
3. âœ… HTML carregado

**ğŸŸ¢ ETAPA 2A - IA IDENTIFICA SELETORES:**
1. Aba "ğŸ¤– ExtraÃ§Ã£o com IA"
2. Descrever campos desejados
3. IA identifica seletores
4. âœ… Seletores salvos em memÃ³ria

**ğŸ”µ ETAPA 1B - NÃƒO PRECISA CARREGAR NADA!**
(Scraping em Massa carrega mÃºltiplas URLs sozinho)

**ğŸŸ¢ ETAPA 2B - APLICAR SELETORES EM MASSA:**
1. Aba "ğŸš€ Scraping em Massa"
2. Marcar "âœ¨ Usar seletores identificados pela IA"
3. Inserir lista de URLs (uma por linha)
4. Clicar "ğŸš€ Iniciar Scraping em Massa"
5. Sistema carrega CADA URL automaticamente
6. Aplica seletores da IA em todas
7. Download CSV/JSON consolidado

---

### 4. Sites JavaScript-heavy (Steam, Amazon):

**ğŸ”µ ETAPA 1 - SALVAR HTML DO NAVEGADOR:**
1. Abrir Steam/Amazon no Chrome/Firefox
2. Ctrl+S (Salvar pÃ¡gina completa) OU Ctrl+U (View Source) â†’ Save
3. Salvar arquivo .html no computador
4. âœ… HTML completo com JavaScript renderizado

**ğŸŸ¢ ETAPA 2A - IA IDENTIFICA SELETORES:**
1. Login no app
2. Sidebar: "ğŸ“ Upload de arquivo HTML"
3. Upload do arquivo salvo
4. Clicar "ğŸ“¥ Processar HTML"
5. Aba "ğŸ¤– ExtraÃ§Ã£o com IA"
6. Descrever campos
7. IA identifica seletores
8. âœ… Seletores salvos

**ğŸŸ¢ ETAPA 2B - PROCESSAR MÃšLTIPLOS HTMLS:**
1. Salvar vÃ¡rios produtos (10, 20 pÃ¡ginas)
2. Aba "ğŸš€ Scraping em Massa"
3. OpÃ§Ã£o "ğŸ“„ Upload de Arquivos HTML"
4. Selecionar TODOS os arquivos de uma vez
5. Marcar "âœ¨ Usar seletores da IA"
6. Processar
7. Download consolidado com nome do arquivo origem

---

### 5. AutomaÃ§Ã£o com Email (Admin):

**ğŸ”µ ETAPA 1 - CONFIGURAR TAREFA (nÃ£o carrega nada):**
1. Login como admin
2. Aba "ğŸ¤– Scraping AutomÃ¡tico"

**ğŸŸ¢ ETAPA 2 - CONFIGURAR EXTRAÃ‡ÃƒO AUTOMÃTICA:**
1. Preencher formulÃ¡rio:
   - Nome: "LanÃ§amentos Semanais Steam"
   - URL Fonte: pÃ¡gina de novidades
   - Site Alvo: Steam
   - MÃ©todo: "HÃ­brido (Python + IA)"
   - Campos: TÃ­tulo, PreÃ§o, Link
   - FrequÃªncia: Semanal
   - Email: SMTP Customizado
     - Servidor: smtp.gmail.com
     - Porta: 587
     - UsuÃ¡rio: seu@gmail.com
     - Senha: senha_app
     - DestinatÃ¡rio: destino@email.com
2. Salvar tarefa
3. Clicar "â–¶ï¸ Executar Agora" para testar
4. Sistema carrega URL automaticamente
5. IA identifica seletores
6. Extrai produtos
7. Mostra preview
8. Envia email com resultados
9. âœ… Tarefa salva para repetiÃ§Ã£o futura

---

### 6. Validar MÃºltiplos Seletores:

**ğŸ”µ ETAPA 1 - CARREGAR HTML:**
1. Login
2. Carregar qualquer pÃ¡gina
3. âœ… HTML em memÃ³ria

**ğŸŸ¢ ETAPA 2 - TESTAR SELETORES:**
1. Aba "âš¡ Validador de Seletores"
2. Colar lista de seletores (CSS e XPath misturados):
   ```
   div.produto
   //h1[@class='titulo']
   span.price
   //a/@href
   ```
3. Clicar "ğŸš€ Testar Todos os Seletores"
4. Ver resultados: tipo detectado, quantos encontrados, preview
5. Download CSV/JSON

---

## ğŸ—ï¸ ARQUITETURA FINAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         USUÃRIO ACESSA O APP                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ” TELA DE LOGIN (app.py)               â”‚
â”‚     - UsuÃ¡rio/Senha                         â”‚
â”‚     - ValidaÃ§Ã£o via Secrets                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ âœ… Autenticado
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ“‚ SIDEBAR - CONTROLE                   â”‚
â”‚     - MÃ©todo de carregamento                â”‚
â”‚     - Carregar URL / Upload HTML            â”‚
â”‚     - Gerenciar Acessos (admin)             â”‚
â”‚     - Gerenciar API Keys (admin)            â”‚
â”‚     - HistÃ³rico de ExecuÃ§Ãµes                â”‚
â”‚     - Logout                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ“‘ 5 ABAS PRINCIPAIS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ğŸ“Š Estrutura HTML                       â”‚
â”‚     - EstatÃ­sticas                          â”‚
â”‚     - Tags/Classes/IDs                      â”‚
â”‚     - Preview HTML                          â”‚
â”‚     - VisualizaÃ§Ã£o renderizada              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. ğŸ¤– ExtraÃ§Ã£o com IA                      â”‚
â”‚     - 3 Provedores (GPT-5, Claude, Gemini)  â”‚
â”‚     - DescriÃ§Ã£o em linguagem natural        â”‚
â”‚     - Auto-identificaÃ§Ã£o de seletores       â”‚
â”‚     - ExtraÃ§Ã£o automÃ¡tica + download        â”‚
â”‚     - CÃ³pia de seletores                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. ğŸš€ Scraping em Massa                    â”‚
â”‚     - URLs mÃºltiplas                        â”‚
â”‚     - Upload de HTMLs                       â”‚
â”‚     - Seletores IA ou custom                â”‚
â”‚     - Progress bar                          â”‚
â”‚     - Resultado consolidado                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. âš¡ Validador de Seletores               â”‚
â”‚     - Teste mÃºltiplo                        â”‚
â”‚     - DetecÃ§Ã£o auto CSS/XPath               â”‚
â”‚     - Resultados em tabela                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. ğŸ¤– Scraping AutomÃ¡tico (admin)          â”‚
â”‚     - Configurar tarefas                    â”‚
â”‚     - Agendamento                           â”‚
â”‚     - Email (SMTP custom)                   â”‚
â”‚     - ExecuÃ§Ã£o manual                       â”‚
â”‚     - HistÃ³rico                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸŒ PROXY SERVER (proxy_server.py)       â”‚
â”‚     - Flask na porta 5001                   â”‚
â”‚     - /proxy?url=...                        â”‚
â”‚     - CORS bypass via corsproxy.io          â”‚
â”‚     - /health para status                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ’¾ PERSISTÃŠNCIA                         â”‚
â”‚     - api_keys.json                         â”‚
â”‚     - authorized_emails.json                â”‚
â”‚     - scraping_tasks.json                   â”‚
â”‚     - scraping_history.json                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ NOTAS FINAIS

### Pontos Fortes:
- âœ… Interface 100% em portuguÃªs
- âœ… MÃºltiplos mÃ©todos de extraÃ§Ã£o
- âœ… IA com 3 provedores principais
- âœ… Proxy para contornar bloqueios
- âœ… AutomaÃ§Ã£o completa com email
- âœ… Upload de HTML para sites JavaScript
- âœ… Gerenciamento hÃ­brido de API keys
- âœ… Sistema de autenticaÃ§Ã£o funcional
- âœ… Admin panel completo

### Melhorias Futuras PossÃ­veis:
- IntegraÃ§Ã£o completa SendGrid/Resend/Gmail
- Agendamento real via Replit Scheduled Deployments
- Dashboard de analytics de scraping
- Export para Google Sheets
- Webhooks para notificaÃ§Ãµes
- Rate limiting configurÃ¡vel
- Retry automÃ¡tico em caso de falha

---

**Este Ã© o PROMPT COMPLETO com TODAS as funcionalidades implementadas no Web Scraper Intuitivo.**
