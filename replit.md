# Web Scraper Intuitivo

## Overview

This project is a Streamlit-based web scraping application designed to provide an intuitive visual interface in Portuguese for extracting data from web pages. Its primary purpose is to enable users to easily configure and execute web scraping tasks, including automated and scheduled operations, without needing to write code. The application supports various extraction methods, AI-assisted selector identification, bulk processing, and data export.

## Recent Changes

### State Management Isolation (October 30, 2025)
- ‚úÖ **Isolamento de Estados**: Sistema de limpeza autom√°tica para evitar mistura de dados
  - **Problema Resolvido**: Dados de opera√ß√µes anteriores n√£o aparecem mais em novos processamentos
  - **Fun√ß√µes Helper**:
    - `reset_single_extraction()`: Limpa resultados de p√°gina √∫nica
    - `reset_multi_url_extraction()`: Limpa resultados multi-URL
  - **Limpeza Autom√°tica**:
    - Ao ativar multi-URL: limpa resultados de p√°gina √∫nica (ai_result, ai_direct_result)
    - Ao desativar multi-URL: limpa resultados multi-URL (multi_url_results, loaded_urls)
    - Ao carregar novas URLs: limpa resultados anteriores
  - **Detec√ß√£o de Mudan√ßa**: Compara estado anterior com atual via `previous_multi_url_mode`
  - **Benef√≠cios UX**:
    - Cada modo opera isoladamente sem interfer√™ncia
    - Workflow mais previs√≠vel e intuitivo
    - Sem confus√£o entre dados de diferentes opera√ß√µes

### AI Direct Data Extraction (October 30, 2025)
- ‚úÖ **Extra√ß√£o Direta com IA**: Novo modo de IA que extrai dados sem identificar seletores
  - **Dois Bot√µes na Tab 3**:
    - **üîç Identificar Seletores com IA**: Modo original (gera seletores reutiliz√°veis)
    - **‚ö° Extrair Dados Direto com IA**: Modo novo (extra√ß√£o direta, mais r√°pido)
  - **Vantagens da Extra√ß√£o Direta**:
    - Mais r√°pido para consultas √∫nicas
    - Mais econ√¥mico (menos tokens)
    - Ideal para p√°ginas que mudam frequentemente
    - N√£o requer aplicar seletores manualmente
  - **Formato de Resposta**:
    - JSON estruturado com campos, valores e status
    - Tabela visual com resultados
    - Downloads em CSV e JSON
  - **Robustez**:
    - Valida√ß√£o de JSON na resposta da IA
    - Fallback para provedores n√£o reconhecidos
    - Tratamento de erros detalhado
  - **Benef√≠cios UX**:
    - Escolhe entre velocidade (direto) ou reutiliza√ß√£o (seletores)
    - Interface clara mostrando diferen√ßa entre os dois modos
    - Resultados imediatos sem etapas intermedi√°rias

### Enhanced Bulk Scraping with Selective Downloads (October 30, 2025)
- ‚úÖ **Scraping em Massa Aprimorado**: Sistema completo de sele√ß√£o e filtragem de resultados
  - **Detec√ß√£o Autom√°tica de Problemas**:
    - Identifica URLs com campos vazios, 'nan', 'none' ou erros
    - M√©tricas visuais: Total URLs, URLs Completas, URLs com Problemas
    - Indicadores visuais (‚úÖ/‚ö†Ô∏è) em cada URL
  - **Sistema de Filtros**:
    - üìã Todas as URLs
    - ‚úÖ Apenas URLs Completas
    - ‚ö†Ô∏è Apenas URLs com Problemas
  - **Sele√ß√£o Individual por URL**:
    - Checkbox em cada URL para marcar/desmarcar
    - Bot√µes "Marcar Todas (Filtradas)" e "Desmarcar Todas"
    - Contador din√¢mico de URLs selecionadas
    - Preview expandable de problemas espec√≠ficos em cada URL
  - **Downloads Seletivos**:
    - CSV/JSON apenas das URLs selecionadas
    - Download individual quando s√≥ 1 URL est√° marcada
    - Nome de arquivo autom√°tico baseado no n√∫mero de URLs
    - Op√ß√£o separada para baixar TODAS as URLs (n√£o filtrado)
  - **Session State Management**:
    - Resultados salvos em `st.session_state.bulk_results`
    - Sele√ß√£o resetada automaticamente a cada novo scraping
    - Todas as URLs novas ficam marcadas por padr√£o
  - **Benef√≠cios UX**:
    - V√™ imediatamente quais URLs tiveram problemas
    - Pode baixar s√≥ as URLs que funcionaram
    - N√£o precisa baixar tudo quando s√≥ quer algumas URLs
    - Filtros r√°pidos para encontrar problemas

### Two-Phase Multi-URL Workflow (October 30, 2025)
- ‚úÖ **Workflow em Duas Etapas**: Refatora√ß√£o completa do Modo Multi-URL para melhor UX
  - **ETAPA 1: Carregar URLs**:
    - Campo de texto para inserir m√∫ltiplas URLs (uma por linha)
    - Bot√£o "üì• Carregar URLs" com barra de progresso
    - Tabela visual com status de cada URL carregada (‚úÖ/‚ùå)
    - Checkboxes para selecionar quais URLs processar com IA
    - URLs carregadas ficam em mem√≥ria (session_state) at√© limpeza manual
  - **ETAPA 2: Processar com IA** (s√≥ aparece ap√≥s carregar URLs):
    - Mostra contador de URLs selecionadas
    - Radio buttons para escolher estrat√©gia:
      - **‚ö° Mesmos seletores (r√°pido)**: IA analisa p√°gina atual e aplica em todas
      - **üéØ Seletores individuais (preciso)**: IA analisa cada URL separadamente
    - Bot√£o "ü§ñ Processar URLs Selecionadas com IA"
    - Barra de progresso durante processamento
  - **Fun√ß√µes Helper Compartilhadas**:
    - `fetch_html(url, extraction_method, timeout)`: Fonte √∫nica de verdade para HTTP requests
    - `load_urls(urls, extraction_method, timeout)`: Carrega m√∫ltiplas URLs com status
  - **Refatora√ß√µes T√©cnicas**:
    - `apply_selectors_to_url()` e `apply_ai_per_url()` agora usam `fetch_html()` internamente
    - Eliminada duplica√ß√£o de c√≥digo de requisi√ß√µes HTTP
    - M√©todo de extra√ß√£o (Python/Proxy) respeitado em todas as opera√ß√µes
  - **Benef√≠cios UX**:
    - Usu√°rio v√™ quais URLs carregaram com sucesso antes de gastar cr√©ditos de IA
    - Pode desmarcar URLs com erro antes de processar
    - Workflow mais intuitivo e previs√≠vel
    - Menos surpresas e mais controle sobre o processo

### Migration to Streamlit Cloud (October 28, 2025)
- ‚úÖ **Simplified API Key Management**: Removed custom API key management panel
  - **Environment-only**: All API keys now configured via Streamlit Secrets
  - **Removed**: Custom API key management UI (not compatible with Streamlit Cloud)
  - **Configuration**: Set API keys in Streamlit Cloud Settings ‚Üí Secrets
  - Required secrets:
    - `ADMIN_USERNAME`: Login username
    - `ADMIN_PASSWORD`: Login password
    - `SESSION_SECRET`: Random string for session encryption
    - `GEMINI_API_KEY` (optional): Google Gemini API key
    - `OPENAI_API_KEY` (optional): OpenAI API key
    - `ANTHROPIC_API_KEY` (optional): Anthropic Claude API key
- ‚úÖ **GitHub Integration**: Project synced to https://github.com/Igor-dev1/web-scraper-intuitivo
- ‚úÖ **Platform Migration**: Moved from Replit to Streamlit Community Cloud for free hosting
- ‚úÖ **Updated Documentation**: Removed references to Replit-specific features

## User Preferences

Preferred communication style: Simple, everyday language in Portuguese (Brasil).

## System Architecture

### Frontend Architecture
- **Framework**: Streamlit for rapid development and data-focused web applications.
- **Layout**: Wide layout with a sidebar for configuration and a tab-based interface for different functionalities.
- **State Management**: Streamlit's session state persists loaded page data (HTML content, parsed soup object, URL) across interactions.
- **UI Components**: Tabbed navigation, real-time data visualization with pandas DataFrames, and CSV/JSON export buttons.

### Backend Architecture
- **Web Scraping Stack**: `requests` for HTTP requests, `BeautifulSoup` with `lxml` parser for HTML parsing and DOM manipulation.
- **Data Processing**: `pandas` for structured data output and tabular visualization.
- **Session State Pattern**: Maintains `html_content`, `soup`, and `url` for the current scraping context.

### Extraction Methods
The application offers various extraction methods:
- **Estrutura HTML**: HTML structure preview with page statistics, tags, classes, and IDs.
- **Seletor CSS**: Extract elements using CSS selectors.
- **XPath**: Extract elements or attributes using XPath expressions.
- **Tag HTML**: Extract all elements of a specified HTML tag.
- **Classe**: Extract elements by CSS class name.
- **ID**: Extract an element by its ID.
- **Avan√ßado**: Multi-attribute extraction with customizable options.
- **Extra√ß√£o com IA**: AI-powered selector identification based on natural language descriptions, supporting multiple AI providers.
  - **Modo Multi-URL**: Apply AI-identified selectors across multiple URLs in batch, with individual and combined download options.
- **Scraping em Massa**: Batch processing of multiple URLs with the same selectors.
- **Teste Universal**: Test multiple mixed CSS and XPath selectors simultaneously.
- **Scraping Autom√°tico**: Automated scraping tasks with scheduling, email notifications, and admin-only access.

### Design Decisions
- **User-Agent Spoofing**: Custom headers to mimic browser behavior and avoid blocking.
- **Error Handling**: HTTP status validation and timeout configuration for robust requests.
- **Parser Choice**: `lxml` chosen for speed and robustness.
- **Localization**: Full Portuguese interface for Brazilian users.
- **Security**: Simple username/password authentication via Streamlit Secrets for restricted access.
- **Simplified API Key Management**: All API keys configured via environment variables (Streamlit Secrets) only.
- **Automated Scraping System**: Intuitive task configuration, flexible scheduling (pre-defined or cron), multi-provider email notifications (SMTP, SendGrid, Resend, Gmail), AI-powered selector identification, CRUD task management, execution history, and persistent data storage (`scraping_tasks.json`, `scraping_history.json`).

## External Dependencies

### Python Libraries
- **streamlit**: Web application framework.
- **requests**: HTTP client.
- **beautifulsoup4**: HTML/XML parsing.
- **lxml**: Faster XML/HTML parser backend.
- **pandas**: Data manipulation.
- **openai**: OpenAI API client for AI assistance.
- **anthropic**: Anthropic API client for AI assistance.
- **google-genai**: Google Gemini API client for AI assistance.

### External Services
- **External websites**: Web pages fetched for scraping.
- **OpenAI API**: For AI-assisted selector identification (requires `OPENAI_API_KEY`).
- **Anthropic API**: For AI-assisted selector identification (requires `ANTHROPIC_API_KEY`).
- **Google Gemini API**: For AI-assisted selector identification (requires `GEMINI_API_KEY`).
- **Email Services**: SMTP, SendGrid, Resend, Gmail for automated task notifications.

### API Key Management
- **Streamlit Secrets**: All API keys and administrative credentials (`ADMIN_USERNAME`, `ADMIN_PASSWORD`, `SESSION_SECRET`) are configured via Streamlit Secrets for security.
- **No Local Storage**: API keys are not stored in local files for compatibility with Streamlit Cloud.

## Deployment

### Streamlit Community Cloud
The application is designed to be deployed on Streamlit Community Cloud:
1. Connect GitHub repository
2. Configure Secrets in Settings
3. Deploy with `app.py` as main file
4. Free hosting with automatic restarts
